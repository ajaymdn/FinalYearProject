{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests to retrive Web Urls example HTML. TXT \n",
    "import requests\n",
    "\n",
    "# Import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import re module for REGEXes\n",
    "import re\n",
    "\n",
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converttotext(url):\n",
    "    headers = {'User-Agent':'Sample Company Name AdminContact@<sample company domain>.com','Accept-Encoding':'gzip, deflate','Host':'www.sec.gov'}\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "\n",
    "    raw_10k = r.text\n",
    "\n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "\n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10k)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10k)]\n",
    "\n",
    "    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10k)]\n",
    "\n",
    "    document = {}\n",
    "    \n",
    "    for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "        if doc_type == '10-K':\n",
    "            document[doc_type] = raw_10k[doc_start:doc_end]\n",
    "\n",
    "    regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;)(1A|1B|7A|7|8)\\.{0,1})|(ITEM\\s(1A|1B|7A|7|8))')\n",
    "\n",
    "    matches = regex.finditer(document['10-K'])\n",
    "\n",
    "    test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "\n",
    "    test_df.columns = ['item', 'start', 'end']\n",
    "    test_df['item'] = test_df.item.str.lower()\n",
    "    test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "    test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "    test_df.replace(' ','',regex=True,inplace=True)\n",
    "    test_df.replace('\\.','',regex=True,inplace=True)\n",
    "    test_df.replace('>','',regex=True,inplace=True)\n",
    "\n",
    "    pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n",
    "    pos_dat.set_index('item', inplace=True)\n",
    "\n",
    "    item_1a_raw = document['10-K'][pos_dat['start'].loc['item1a']:pos_dat['start'].loc['item1b']]\n",
    "\n",
    "    item_1a_content = BeautifulSoup(item_1a_raw, 'lxml')\n",
    "\n",
    "    item_1a_text = item_1a_content.get_text()\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'(Item\\s*\\d+[A-Za-z]*)\\.', r'\\1. ', text)\n",
    "        text = re.sub(r'(?<=[.!?])(?=\\S)', r' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    item_1a_text_cleaned = clean_text(item_1a_text)\n",
    "\n",
    "    item_1a_text_cleaned = re.sub(r'(?<=[.!?])(?=\\S)', r' ', item_1a_text_cleaned)\n",
    "\n",
    "\n",
    "    sentence_splitter = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<!\\.\\.\\.)(?<=\\.|\\?)\\s')\n",
    "\n",
    "    sentences = sentence_splitter.split(item_1a_text_cleaned)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Environmental Score: 0.9692671895027161\n",
      "Average Social Score: 0.9106579795479774\n",
      "Average Governance Score: 0.7698687016963959\n"
     ]
    }
   ],
   "source": [
    "sentences = converttotext('https://www.sec.gov/Archives/edgar/data/0000789019/000156459020034944/0001564590-20-034944.txt')\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')\n",
    "nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n",
    "results = nlp(sentences)\n",
    "\n",
    "environment_score = [entry['score'] for entry in results if entry['label'] == 'Environmental']\n",
    "social_score = [entry['score'] for entry in results if entry['label'] == 'Social']\n",
    "governance_score = [entry['score'] for entry in results if entry['label'] == 'Governance']\n",
    "\n",
    "environment_score_avg = sum(environment_score) / len(environment_score) if environment_score else 0\n",
    "social_score_avg = sum(social_score) / len(social_score) if social_score else 0\n",
    "governance_score_avg = sum(governance_score) / len(governance_score) if governance_score else 0\n",
    "\n",
    "print(f'Average Environmental Score: {environment_score_avg}')\n",
    "print(f'Average Social Score: {social_score_avg}')\n",
    "print(f'Average Governance Score: {governance_score_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
